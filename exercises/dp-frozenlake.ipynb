{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "```\n",
    "         Copyright Rein Halbersma 2020-2021.\n",
    "Distributed under the Boost Software License, Version 1.0.\n",
    "   (See accompanying file LICENSE_1_0.txt or copy at\n",
    "         http://www.boost.org/LICENSE_1_0.txt)\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Dynamic programming for the Frozen Lake"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from doctrina.algorithms import dp\n",
    "from doctrina.spaces import state_table\n",
    "\n",
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = W = int(np.sqrt(env.nS))\n",
    "env.observation_shape = (H, W)\n",
    "terminal = env.nS\n",
    "Reward = np.array([0.0, 1.0])\n",
    "nR = len(Reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p(s', r|s, a): probability of transition to state s' with reward r, from state s and action a\n",
    "P_tensor = np.zeros((env.nS + 1, env.nA, env.nS + 1, nR))\n",
    "P_tensor[terminal, :, terminal, 0] = 1\n",
    "for s, a in product(range(env.nS), range(env.nA)):\n",
    "    for prob, next, reward, done in env.P[s][a]:\n",
    "        P_tensor[s, a, terminal if done else next, int(reward)] += prob\n",
    "assert np.isclose(P_tensor.sum(axis=(2, 3)), 1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p(s'|s, a): probability of transition to state s', from state s taking action a\n",
    "transition = P_tensor.sum(axis=3)\n",
    "assert np.isclose(transition.sum(axis=2), 1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r(s, a): expected immediate reward from state s after action a\n",
    "reward = P_tensor.sum(axis=2) @ Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.nS += 1\n",
    "env.transition = transition\n",
    "env.reward = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0 3 3 3]\n [0 0 0 0]\n [3 1 0 0]\n [0 2 1 0]]\n[[0.82352917 0.8235291  0.82352904 0.82352901]\n [0.82352919 0.         0.5294116  0.        ]\n [0.82352923 0.82352927 0.76470576 0.        ]\n [0.         0.88235284 0.94117642 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "policy, V, *_ = dp.V_policy_iter(env)\n",
    "print(state_table(policy[:-1], env))\n",
    "print(state_table(V[:-1], env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0 3 3 3]\n [0 0 0 0]\n [3 1 0 0]\n [0 2 1 0]]\n[[0.82352917 0.8235291  0.82352904 0.82352901]\n [0.82352919 0.         0.5294116  0.        ]\n [0.82352923 0.82352927 0.76470576 0.        ]\n [0.         0.88235284 0.94117642 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "policy, Q, *_ = dp.Q_policy_iter(env)\n",
    "V = Q.max(axis=1)\n",
    "print(state_table(policy[:-1], env))\n",
    "print(state_table(V[:-1], env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0 3 3 3]\n [0 0 0 0]\n [3 1 0 0]\n [0 2 1 0]]\n[[0.82352918 0.8235291  0.82352904 0.82352901]\n [0.82352919 0.         0.5294116  0.        ]\n [0.82352923 0.82352927 0.76470576 0.        ]\n [0.         0.88235284 0.94117642 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "policy, V, *_ = dp.V_value_iter(env)\n",
    "print(state_table(policy[:-1], env))\n",
    "print(state_table(V[:-1], env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0 3 3 3]\n [0 0 0 0]\n [3 1 0 0]\n [0 2 1 0]]\n[[0.82352918 0.8235291  0.82352904 0.82352901]\n [0.82352919 0.         0.5294116  0.        ]\n [0.82352923 0.82352927 0.76470576 0.        ]\n [0.         0.88235284 0.94117642 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "policy, Q, *_ = dp.Q_value_iter(env)\n",
    "V = Q.max(axis=1)\n",
    "print(state_table(policy[:-1], env))\n",
    "print(state_table(V[:-1], env))"
   ]
  },
  {
   "source": [
    "## Purrformance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10.2 ms ± 85.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dp.Q_value_iter(env)"
   ]
  }
 ]
}