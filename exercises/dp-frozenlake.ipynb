{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "```\n",
    "         Copyright Rein Halbersma 2020-2021.\n",
    "Distributed under the Boost Software License, Version 1.0.\n",
    "   (See accompanying file LICENSE_1_0.txt or copy at\n",
    "         http://www.boost.org/LICENSE_1_0.txt)\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Dynamic programming for the Frozen Lake\n",
    "An implementation of the dynamic programming assignment of the [Udacity Deep Reinforcement Learning Nanodegree](https://github.com/udacity/deep-reinforcement-learning/blob/master/dynamic-programming/Dynamic_Programming_Solution.ipynb)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from doctrina.algorithms import dp\n",
    "from doctrina.spaces import state_table\n",
    "\n",
    "env = gym.make('FrozenLake-v0', is_slippery=True)"
   ]
  },
  {
   "source": [
    "## Part -1: Adapt the Gym environment\n",
    "> The reasonable man adapts himself to the world: the unreasonable one persists in trying to adapt the world to himself. Therefore all progress depends on the        unreasonable man.”\n",
    ">\n",
    ">    ― George Bernard Shaw, Man and Superman"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Expand the state space to include a terminal state\n",
    "We must be careful to distinguish episodic tasks from continuing tasks. For episodic tasks (such as `FrozenLake-v0`), Sutton & Barto (p.54) recommend introducing a terminal state in which all episodes end. This distinguishes the set of all nonterminal states, denoted $S$, from the set of all states plus the terminal state, denoted $S^{+}$. \n",
    "\n",
    "The authors of the Frozen Lake environment did not follow this advice and instead encode the terminal nature of the goal state and the holes in the lake by the `done` flag. For the Frozen Lake environment this still leads to the correct results. But for more general environments this is not necessarily the case because the terminal nature of a state can be action-dependent. \n",
    "\n",
    "In the Blackjack environment, a hand will always terminate after standing, but after hitting it depends on whether the player busts or not. For such environments an explicit terminal state is required to assign a unique value to each state. To avoid having to analyze a specific environment's properties beforehand, we always create a terminal state for episodic tasks, even when it is not strictly necessary.\n",
    "\n",
    "For continuing tasks (such as Jack's Car Rental), no terminal state is required but one does need to set the discount parameter $\\gamma \\lt 1$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = W = int(np.sqrt(env.nS))\n",
    "env.observation_shape = (H, W)\n",
    "terminal = env.nS\n",
    "env.nSp = env.nS + 1"
   ]
  },
  {
   "source": [
    "### Extract unique rewards\n",
    "We need to know the (sorted) vector of unique rewards in order to determine the size of the dense tensor representing `env.P`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reward = np.unique([\n",
    "    t[2]\n",
    "    for s, a in product(range(env.nS), range(env.nA))\n",
    "    for t in env.P[s][a]\n",
    "])\n",
    "nR = len(Reward)"
   ]
  },
  {
   "source": [
    "### Create a dense version of `env.P`\n",
    "For reasons of both clarity and performance, it's more convenient to change the dictionary of dictionaries of lists `env.P` into a dense tensor that directly models equation (3.2) in Sutton & Barto. This allows the use of NumPy matrix multiplication when implementing the Bellman-equation. The transformation between the two equivalent representations is a relative straightforward exercise that can be applied to any model-based Gym environment found on the internet. The transformation is reminiscent of the transformation between adjacency lists and adjacency matrices for graph representations."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equation (3.2) in Sutton & Barto (p.48):\n",
    "# p(s', r|s, a) = probability of transition to state s' with reward r, from state s and action a.\n",
    "P_tensor = np.zeros((env.nSp, env.nA, env.nSp, nR))\n",
    "P_tensor[terminal, :, terminal, 0] = 1\n",
    "for s, a in product(range(env.nS), range(env.nA)):\n",
    "    for prob, next, reward, done in env.P[s][a]:\n",
    "        P_tensor[s, a, terminal if done else next, int(reward)] += prob\n",
    "# Equation (3.3) in Sutton & Barto (p.48):\n",
    "assert np.isclose(P_tensor.sum(axis=(2, 3)), 1).all()"
   ]
  },
  {
   "source": [
    "### Avoid redundant computations (aka Once And Only Once)\n",
    "The two terms in the Bellman-equation both contain summations that are state-independent. This redundancy can be avoided by pre-computing a separate transition tensor and a a reward matrix as discussed in Sutton & Barto (p.49). For the reward matrix, we need a (sorted) vector of unique rewards."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equation (3.4) in Sutton & Barto (p.49):\n",
    "# p(s'|s, a) = probability of transition to state s', from state s taking action a.\n",
    "env.transition = P_tensor.sum(axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equation (3.5) in Sutton & Barto (p.49):\n",
    "# r(s, a) = expected immediate reward from state s after action a.\n",
    "env.reward = P_tensor.sum(axis=2) @ Reward"
   ]
  },
  {
   "source": [
    "## Part 0: Explore FrozenLakeEnv"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Discrete(16)\nDiscrete(4)\n16\n4\n"
     ]
    }
   ],
   "source": [
    "# print the state space and action space\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "\n",
    "# print the total number of states and actions\n",
    "print(env.nS)\n",
    "print(env.nA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(0.3333333333333333, 1, 0.0, False),\n",
       " (0.3333333333333333, 0, 0.0, False),\n",
       " (0.3333333333333333, 5, 0.0, True)]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "env.P[1][0]"
   ]
  },
  {
   "source": [
    "## Part 1: Iterative Policy Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.01394 0.01163 0.02095 0.01048]\n [0.01625 0.      0.04075 0.     ]\n [0.03481 0.08817 0.14205 0.     ]\n [0.      0.17582 0.43929 0.     ]]\n9.372906270566084e-09 74\n"
     ]
    }
   ],
   "source": [
    "random_policy = dp.policy_init_stoch(env)\n",
    "V, delta, iter = dp.V_policy_eval_stoch_sync(env, random_policy)\n",
    "print(state_table(V[:-1], env).round(5))\n",
    "print(delta, iter)"
   ]
  },
  {
   "source": [
    "## Part 2: Obtain $q_\\pi$ from $v_\\pi$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.01470938 0.01393976 0.01393976 0.01317013]\n [0.00852355 0.0116309  0.01086128 0.01550787]\n [0.02444513 0.02095296 0.02406032 0.01435344]\n [0.01047648 0.01047648 0.00698431 0.01396863]\n [0.02166486 0.01701827 0.01624865 0.0100628 ]\n [0.         0.         0.         0.        ]\n [0.05433537 0.04735105 0.05433537 0.00698432]\n [0.         0.         0.         0.        ]\n [0.01701827 0.04099204 0.03480619 0.04640825]\n [0.07020885 0.1175599  0.10595784 0.05895311]\n [0.18940421 0.17582036 0.16001423 0.04297382]\n [0.         0.         0.         0.        ]\n [0.         0.         0.         0.        ]\n [0.08799676 0.20503718 0.23442715 0.17582036]\n [0.25238823 0.53837051 0.52711477 0.43929117]\n [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "Q = dp.Q_from_V_sync(env, V, gamma=1.)\n",
    "print(Q[:-1])"
   ]
  },
  {
   "source": [
    "## Part 3: Policy Improvement"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1.   0.   0.   0.  ]\n [0.   0.   0.   1.  ]\n [1.   0.   0.   0.  ]\n [0.   0.   0.   1.  ]\n [1.   0.   0.   0.  ]\n [0.25 0.25 0.25 0.25]\n [0.5  0.   0.5  0.  ]\n [0.25 0.25 0.25 0.25]\n [0.   0.   0.   1.  ]\n [0.   1.   0.   0.  ]\n [1.   0.   0.   0.  ]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.   0.   1.   0.  ]\n [0.   1.   0.   0.  ]\n [0.25 0.25 0.25 0.25]]\n"
     ]
    }
   ],
   "source": [
    "policy = dp.V_policy_impr_stoch(env, V, gamma=1.)\n",
    "print(policy[:-1])"
   ]
  },
  {
   "source": [
    "## Part 4: Policy Iteration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.25 0.25 0.25 0.25]\n [0.   0.   0.   1.  ]\n [0.   0.   0.   1.  ]\n [0.   0.   0.   1.  ]\n [1.   0.   0.   0.  ]\n [0.25 0.25 0.25 0.25]\n [0.5  0.   0.5  0.  ]\n [0.25 0.25 0.25 0.25]\n [0.   0.   0.   1.  ]\n [0.   1.   0.   0.  ]\n [1.   0.   0.   0.  ]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.   0.   1.   0.  ]\n [0.   1.   0.   0.  ]\n [0.25 0.25 0.25 0.25]]\n[[0.82353 0.82353 0.82353 0.82353]\n [0.82353 0.      0.52941 0.     ]\n [0.82353 0.82353 0.76471 0.     ]\n [0.      0.88235 0.94118 0.     ]]\n{'delta': 9.513950849360242e-09, 'evaluations': 1078, 'improvements': 4}\n"
     ]
    }
   ],
   "source": [
    "policy, V, info = dp.V_policy_iter(env, stoch=True)\n",
    "print(policy[:-1])\n",
    "print(state_table(V[:-1], env).round(5))\n",
    "print(info)"
   ]
  },
  {
   "source": [
    "## Part 5: Truncated Policy Iteration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.25 0.25 0.25 0.25]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.5  0.   0.5  0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.   1.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "[[0.82353 0.82353 0.82353 0.82353]\n",
      " [0.82353 0.      0.52941 0.     ]\n",
      " [0.82353 0.82353 0.76471 0.     ]\n",
      " [0.      0.88235 0.94118 0.     ]]\n",
      "{'delta': 9.935870903809985e-09, 'evaluations': 830, 'improvements': 415}\n"
     ]
    }
   ],
   "source": [
    "policy, V, info = dp.V_policy_iter(env, stoch=True, maxiter=2)\n",
    "print(policy[:-1])\n",
    "print(state_table(V[:-1], env).round(5))\n",
    "print(info)"
   ]
  },
  {
   "source": [
    "## Part 6: Value Iteration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.25 0.25 0.25 0.25]\n [0.   0.   0.   1.  ]\n [0.   0.   0.   1.  ]\n [0.   0.   0.   1.  ]\n [1.   0.   0.   0.  ]\n [0.25 0.25 0.25 0.25]\n [0.5  0.   0.5  0.  ]\n [0.25 0.25 0.25 0.25]\n [0.   0.   0.   1.  ]\n [0.   1.   0.   0.  ]\n [1.   0.   0.   0.  ]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.   0.   1.   0.  ]\n [0.   1.   0.   0.  ]\n [0.25 0.25 0.25 0.25]]\n[[0.82353 0.82353 0.82353 0.82353]\n [0.82353 0.      0.52941 0.     ]\n [0.82353 0.82353 0.76471 0.     ]\n [0.      0.88235 0.94118 0.     ]]\n{'delta': 9.96687876675395e-09, 'iter': 618}\n"
     ]
    }
   ],
   "source": [
    "policy, V, info = dp.V_value_iter(env, stoch=True)\n",
    "print(policy[:-1])\n",
    "print(state_table(V[:-1], env).round(5))\n",
    "print(info)"
   ]
  },
  {
   "source": [
    "## Part 7: Purrformance (aka The Need for Speed)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "9.65 ms ± 192 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dp.V_value_iter(env, stoch=True)"
   ]
  }
 ]
}