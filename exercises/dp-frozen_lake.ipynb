{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "```\n",
    "         Copyright Rein Halbersma 2020-2021.\n",
    "Distributed under the Boost Software License, Version 1.0.\n",
    "   (See accompanying file LICENSE_1_0.txt or copy at\n",
    "         http://www.boost.org/LICENSE_1_0.txt)\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Dynamic programming for the Frozen Lake\n",
    "An implementation of the dynamic programming assignment of the [Udacity Deep Reinforcement Learning Nanodegree](https://github.com/udacity/deep-reinforcement-learning/blob/master/dynamic-programming/Dynamic_Programming_Solution.ipynb)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from doctrina.algorithms import dp\n",
    "\n",
    "env = gym.make('FrozenLake-v0', is_slippery=True)"
   ]
  },
  {
   "source": [
    "## Part -1: Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Infer the frozen lake's height and width\n",
    "To plot value functions as 2D tables, we need to know the lake's height and width."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = W = int(np.sqrt(env.nS))\n",
    "env.observation_shape = (H, W)"
   ]
  },
  {
   "source": [
    "### Avoid redundant computations (aka Once And Only Once)\n",
    "To avoid redundant computations, we pre-compute a separate transition tensor and a reward matrix as discussed in Sutton & Barto (p.49)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Equation (3.4) in Sutton & Barto (p.49):\n",
    "# p(s'|s, a) = probability of transition to state s', from state s taking action a.\n",
    "env.transition = np.zeros((env.nS, env.nA, env.nS))\n",
    "\n",
    "# Equation (3.5) in Sutton & Barto (p.49):\n",
    "# r(s, a) = expected immediate reward from state s after action a.\n",
    "env.reward = np.zeros((env.nS, env.nA))\n",
    "\n",
    "# Initialize the transition and reward tensors.\n",
    "for s in env.P.keys():\n",
    "    for a in env.P[s].keys():\n",
    "        for prob, next, reward, done in env.P[s][a]:\n",
    "            # Exclude transitions to the terminal state.\n",
    "            if not done:\n",
    "                env.transition[s, a, next] += prob\n",
    "            env.reward[s, a] += prob * reward"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 3,
   "outputs": []
  },
  {
   "source": [
    "## Part 0: Explore FrozenLakeEnv"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Discrete(16)\nDiscrete(4)\n16\n4\n"
     ]
    }
   ],
   "source": [
    "# print the state space and action space\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "\n",
    "# print the total number of states and actions\n",
    "print(env.nS)\n",
    "print(env.nA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(0.3333333333333333, 1, 0.0, False),\n",
       " (0.3333333333333333, 0, 0.0, False),\n",
       " (0.3333333333333333, 5, 0.0, True)]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "env.P[1][0]"
   ]
  },
  {
   "source": [
    "## Part 1: Iterative Policy Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "8.452946889322965e-09 57\n",
      "[[0.01394 0.01163 0.02095 0.01048]\n",
      " [0.01625 0.      0.04075 0.     ]\n",
      " [0.03481 0.08817 0.14205 0.     ]\n",
      " [0.      0.17582 0.43929 0.     ]]\n"
     ]
    }
   ],
   "source": [
    "random_policy = dp.policy_init(env)\n",
    "print(random_policy)\n",
    "V, delta, iter = dp.V_policy_eval(env, random_policy)\n",
    "print(delta, iter)\n",
    "print(V.reshape(env.observation_shape).round(5))"
   ]
  },
  {
   "source": [
    "## Part 2: Obtain $q_\\pi$ from $v_\\pi$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.0147094  0.01393978 0.01393978 0.01317015]\n [0.00852356 0.01163091 0.0108613  0.01550788]\n [0.02444514 0.02095298 0.02406033 0.01435346]\n [0.01047649 0.01047649 0.00698432 0.01396865]\n [0.02166487 0.01701828 0.01624865 0.01006281]\n [0.         0.         0.         0.        ]\n [0.05433538 0.04735105 0.05433538 0.00698432]\n [0.         0.         0.         0.        ]\n [0.01701828 0.04099204 0.03480619 0.04640826]\n [0.07020885 0.11755991 0.10595784 0.05895312]\n [0.18940421 0.17582037 0.16001424 0.04297382]\n [0.         0.         0.         0.        ]\n [0.         0.         0.         0.        ]\n [0.08799677 0.20503718 0.23442716 0.17582037]\n [0.25238823 0.53837051 0.52711478 0.43929118]\n [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "Q = dp.Q_from_V(env, V)\n",
    "print(Q)"
   ]
  },
  {
   "source": [
    "## Part 3: Policy Improvement"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1.   0.   0.   0.  ]\n [0.   0.   0.   1.  ]\n [1.   0.   0.   0.  ]\n [0.   0.   0.   1.  ]\n [1.   0.   0.   0.  ]\n [0.25 0.25 0.25 0.25]\n [0.5  0.   0.5  0.  ]\n [0.25 0.25 0.25 0.25]\n [0.   0.   0.   1.  ]\n [0.   1.   0.   0.  ]\n [1.   0.   0.   0.  ]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.   0.   1.   0.  ]\n [0.   1.   0.   0.  ]\n [0.25 0.25 0.25 0.25]]\n"
     ]
    }
   ],
   "source": [
    "policy = dp.V_policy_impr(env, V)\n",
    "print(policy)"
   ]
  },
  {
   "source": [
    "## Part 4: Policy Iteration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'delta': 9.670292233643352e-09, 'evaluations': 798, 'improvements': 3}\n[[0.25 0.25 0.25 0.25]\n [0.   0.   0.   1.  ]\n [0.   0.   0.   1.  ]\n [0.   0.   0.   1.  ]\n [1.   0.   0.   0.  ]\n [0.25 0.25 0.25 0.25]\n [0.5  0.   0.5  0.  ]\n [0.25 0.25 0.25 0.25]\n [0.   0.   0.   1.  ]\n [0.   1.   0.   0.  ]\n [1.   0.   0.   0.  ]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.   0.   1.   0.  ]\n [0.   1.   0.   0.  ]\n [0.25 0.25 0.25 0.25]]\n[[0.82353 0.82353 0.82353 0.82353]\n [0.82353 0.      0.52941 0.     ]\n [0.82353 0.82353 0.76471 0.     ]\n [0.      0.88235 0.94118 0.     ]]\n"
     ]
    }
   ],
   "source": [
    "policy, V, info = dp.V_policy_iter(env)\n",
    "print(info)\n",
    "print(policy)\n",
    "print(V.reshape(env.observation_shape).round(5))"
   ]
  },
  {
   "source": [
    "## Part 5: Truncated Policy Iteration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'delta': 9.980803183928799e-09, 'evaluations': 620, 'improvements': 310}\n[[0.25 0.25 0.25 0.25]\n [0.   0.   0.   1.  ]\n [0.   0.   0.   1.  ]\n [0.   0.   0.   1.  ]\n [1.   0.   0.   0.  ]\n [0.25 0.25 0.25 0.25]\n [0.5  0.   0.5  0.  ]\n [0.25 0.25 0.25 0.25]\n [0.   0.   0.   1.  ]\n [0.   1.   0.   0.  ]\n [1.   0.   0.   0.  ]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.   0.   1.   0.  ]\n [0.   1.   0.   0.  ]\n [0.25 0.25 0.25 0.25]]\n[[0.82353 0.82353 0.82353 0.82353]\n [0.82353 0.      0.52941 0.     ]\n [0.82353 0.82353 0.76471 0.     ]\n [0.      0.88235 0.94118 0.     ]]\n"
     ]
    }
   ],
   "source": [
    "policy, V, info = dp.V_policy_iter(env, maxiter=2)\n",
    "print(info)\n",
    "print(policy)\n",
    "print(V.reshape(env.observation_shape).round(5))"
   ]
  },
  {
   "source": [
    "## Part 6: Value Iteration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'delta': 9.980803183928799e-09, 'evaluations': 620, 'improvements': 310}\n[[0.25 0.25 0.25 0.25]\n [0.   0.   0.   1.  ]\n [0.   0.   0.   1.  ]\n [0.   0.   0.   1.  ]\n [1.   0.   0.   0.  ]\n [0.25 0.25 0.25 0.25]\n [0.5  0.   0.5  0.  ]\n [0.25 0.25 0.25 0.25]\n [0.   0.   0.   1.  ]\n [0.   1.   0.   0.  ]\n [1.   0.   0.   0.  ]\n [0.25 0.25 0.25 0.25]\n [0.25 0.25 0.25 0.25]\n [0.   0.   1.   0.  ]\n [0.   1.   0.   0.  ]\n [0.25 0.25 0.25 0.25]]\n[[0.82353 0.82353 0.82353 0.82353]\n [0.82353 0.      0.52941 0.     ]\n [0.82353 0.82353 0.76471 0.     ]\n [0.      0.88235 0.94118 0.     ]]\n"
     ]
    }
   ],
   "source": [
    "print(info)\n",
    "policy, V, info = dp.V_value_iter(env)\n",
    "print(policy)\n",
    "print(V.reshape(env.observation_shape).round(5))"
   ]
  },
  {
   "source": [
    "## Part 7: Time to solution (aka The Need for Speed)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "V-policy iteration with format=stoch, method=async: 166 ms ± 1.7 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Q-policy iteration with format=stoch, method=async: 642 ms ± 2.73 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "V-policy iteration with format=deter, method=async: 116 ms ± 397 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Q-policy iteration with format=deter, method=async: 1.18 s ± 20.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "V-policy iteration with format=stoch, method=sync: 19.4 ms ± 386 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Q-policy iteration with format=stoch, method=sync: 19.7 ms ± 147 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "V-policy iteration with format=deter, method=sync: 37.3 ms ± 312 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Q-policy iteration with format=deter, method=sync: 37.5 ms ± 803 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "V-policy iteration with format=stoch, method=solve: 522 µs ± 6.06 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "Q-policy iteration with format=stoch, method=solve: 1.13 ms ± 206 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "V-policy iteration with format=deter, method=solve: 807 µs ± 5.14 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "Q-policy iteration with format=deter, method=solve: 2.54 ms ± 589 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "for method in ('async', 'sync', 'solve'):\n",
    "    for format in ('stoch', 'deter'):\n",
    "        print(f'V-policy iteration with format={format}, method={method}: ', end='')\n",
    "        %timeit dp.V_policy_iter(env, format=format, method=method)\n",
    "        print(f'Q-policy iteration with format={format}, method={method}: ', end='')\n",
    "        %timeit dp.Q_policy_iter(env, format=format, method=method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "V-value iteration with format=stoch, method=async: 85 ms ± 253 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Q-value iteration with format=stoch, method=async: 370 ms ± 3.71 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "V-value iteration with format=deter, method=async: 87.2 ms ± 1.16 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Q-value iteration with format=deter, method=async: 379 ms ± 2.56 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "V-value iteration with format=stoch, method=sync: 10.8 ms ± 55.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Q-value iteration with format=stoch, method=sync: 11 ms ± 54.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "V-value iteration with format=deter, method=sync: 10.6 ms ± 108 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Q-value iteration with format=deter, method=sync: 10.7 ms ± 134 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "for method in ('async', 'sync'):\n",
    "    for format in ('stoch', 'deter'):\n",
    "        print(f'V-value iteration with format={format}, method={method}: ', end='')\n",
    "        %timeit dp.V_value_iter(env, format=format, method=method)\n",
    "        print(f'Q-value iteration with format={format}, method={method}: ', end='')\n",
    "        %timeit dp.Q_value_iter(env, format=format, method=method)"
   ]
  }
 ]
}