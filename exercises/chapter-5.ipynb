{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "```\n",
    "         Copyright Rein Halbersma 2020-2021.\n",
    "Distributed under the Boost Software License, Version 1.0.\n",
    "   (See accompanying file LICENSE_1_0.txt or copy at\n",
    "         http://www.boost.org/LICENSE_1_0.txt)\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Chapter 5 Monte Carlo Methods"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 5.1 Monte Carlo Prediction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from doctrina.algorithms import dp, mc, td\n",
    "from doctrina.spaces import shape, size\n",
    "from doctrina.utils import one_hot_encode\n",
    "import gym_blackjack_v1 as bj\n",
    "\n",
    "env = gym.make('Blackjack-v1')\n",
    "env.seed(47110815)"
   ]
  },
  {
   "source": [
    "**Example 5.1: Blackjack**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider the policy that sticks if the player’s sum is 20 or 21, and otherwise hits.\n",
    "stand_on_20 = np.full(env.observation_shape, bj.Action.HIT)\n",
    "stand_on_20[bj.Hand.H20:(bj.Hand.H21 + 1), :] = bj.Action.STAND\n",
    "stand_on_20[bj.Hand.S20:(bj.Hand.BJ  + 1), :] = bj.Action.STAND\n",
    "stand_on_20 = stand_on_20.reshape(env.nS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In any event, after 500,000 games the value function is very well approximated.\n",
    "runs = [ 10_000, 500_000 ]\n",
    "Vs, *_ = zip(*[ \n",
    "    #mc.V_predict_ev(env, stand_on_20, num_episodes, format='deter')\n",
    "    td.V_predict(env, stand_on_20, num_episodes, format='deter') \n",
    "    for num_episodes in runs \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    'vmin': min(env.reward_range), \n",
    "    'vmax': max(env.reward_range), \n",
    "    'cmap': sns.color_palette('coolwarm'), \n",
    "    'center': 0.,\n",
    "    'annot': True, \n",
    "    'xticklabels': bj.card_labels\n",
    "}\n",
    "\n",
    "hands = [\n",
    "    np.arange(bj.Hand.S12, bj.Hand.BJ  + 1),\n",
    "    np.arange(bj.Hand.H12, bj.Hand.H21 + 1)\n",
    "]\n",
    "\n",
    "yticklabels = [ \n",
    "    np.array(bj.hand_labels)[hands[no_usable_ace]] \n",
    "    for no_usable_ace in range(2)\n",
    "]\n",
    "\n",
    "axopts = {\n",
    "    'xlabel': 'Dealer showing',\n",
    "    'ylabel': 'Player sum'\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=len(runs))\n",
    "fig.suptitle(\n",
    "    \"\"\"\n",
    "    Figure 5.1: Approximate state-value functions for the blackjack policy that\n",
    "    sticks only on 20 or 21, computed by Monte Carlo policy evaluation.\n",
    "    \"\"\"\n",
    ")\n",
    "rows = [ 'Usable ace', 'No usable ace']\n",
    "cols = [ f'After {episodes:,} episodes' for episodes in runs ]\n",
    "\n",
    "# https://stackoverflow.com/questions/25812255/row-and-column-headers-in-matplotlibs-subplots\n",
    "pad = 5 # in points\n",
    "\n",
    "for ax, row in zip(axes[:,0], rows):\n",
    "    ax.annotate(\n",
    "        row, xy=(0, 0.5), xytext=(-ax.yaxis.labelpad - pad, 0),\n",
    "        xycoords=ax.yaxis.label, textcoords='offset points',\n",
    "        size='large', ha='right', va='center', rotation=90\n",
    "    )\n",
    "\n",
    "for ax, col in zip(axes[0,:], cols):\n",
    "    ax.annotate(\n",
    "        col, xy=(0.5, 1), xytext=(0, pad),\n",
    "        xycoords='axes fraction', textcoords='offset points',\n",
    "        size='large', ha='center', va='baseline'\n",
    "    )\n",
    "\n",
    "for i, h in enumerate(hands):\n",
    "    for r, _ in enumerate(runs):\n",
    "        sns.heatmap(Vs[r].reshape(env.observation_shape)[h, :], yticklabels=yticklabels[i], ax=axes[i, r], **options).set(**axopts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    Vs[1].reshape(env.observation_shape),\n",
    "    index=bj.hand_labels, \n",
    "    columns=bj.card_labels\n",
    ").round(4)"
   ]
  },
  {
   "source": [
    "## 5.3 Monte Carlo Control"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**Example 5.3: Solving Blackjack**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 500_000\n",
    "Q, N = mc.Q_control_ev(env, num_episodes, policy0=one_hot_encode(stand_on_20))\n",
    "#Q = td.Q_learning(env, num_episodes)\n",
    "policy = Q.argmax(axis=-1)\n",
    "V = Q.max(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2)\n",
    "fig.suptitle(\n",
    "    \"\"\"\n",
    "    Figure 5.2: The optimal policy and state-value function for blackjack, found by Monte Carlo ES. \n",
    "    The state-value function shown was computed from the action-value function found by Monte Carlo ES.\n",
    "    \"\"\"\n",
    ")\n",
    "rows = [ 'Usable ace', 'No usable ace']\n",
    "cols = [ 'Optimal policy', 'Optimal state-value function' ]\n",
    "\n",
    "# https://stackoverflow.com/questions/25812255/row-and-column-headers-in-matplotlibs-subplots\n",
    "pad = 5 # in points\n",
    "\n",
    "for ax, row in zip(axes[:,0], rows):\n",
    "    ax.annotate(\n",
    "        row, xy=(0, 0.5), xytext=(-ax.yaxis.labelpad - pad, 0),\n",
    "        xycoords=ax.yaxis.label, textcoords='offset points',\n",
    "        size='large', ha='right', va='center', rotation=90\n",
    "    )\n",
    "\n",
    "for ax, col in zip(axes[0,:], cols):\n",
    "    ax.annotate(\n",
    "        col, xy=(0.5, 1), xytext=(0, pad),\n",
    "        xycoords='axes fraction', textcoords='offset points',\n",
    "        size='large', ha='center', va='baseline'\n",
    "    )\n",
    "\n",
    "for i, h in enumerate(hands):\n",
    "    sns.heatmap(policy.reshape(env.observation_shape)[h, :], yticklabels=yticklabels[i], ax=axes[i, 0], **options).set(**axopts)\n",
    "    sns.heatmap(     V.reshape(env.observation_shape)[h, :], yticklabels=yticklabels[i], ax=axes[i, 1], **options).set(**axopts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    V.reshape(env.observation_shape), \n",
    "    index=bj.hand_labels, \n",
    "    columns=bj.card_labels\n",
    ").round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    policy.reshape(env.observation_shape), \n",
    "    index=bj.hand_labels, \n",
    "    columns=bj.card_labels\n",
    ").applymap(lambda a: bj.action_labels[a]).replace({'S': ' '})"
   ]
  },
  {
   "source": [
    "## 5.5 Off-policy Prediction via Importance Sampling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**Example 5.4: Off-policy Estimation of a Blackjack State Value**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  1%|▏         | 1385348/100000000 [00:38<45:36, 36042.16it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-38436c650522>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# using the target policy and averaging their returns).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mepisodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100_000_000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV_predict_ev\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/doctrina/src/doctrina/algorithms/mc.py\u001b[0m in \u001b[0;36mV_predict_ev\u001b[0;34m(env, policy, num_episodes, format, gamma, start, V0, N0)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mG\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mN\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# We evaluated the state in which the dealer is showing a deuce, \n",
    "# the sum of the player’s cards is 13, and the player has a usable ace \n",
    "# (that is, the player holds an ace and a deuce, or equivalently three aces).\n",
    "start = np.ravel_multi_index(\n",
    "    (bj.Hand.S13, bj.Card._2), \n",
    "    env.observation_shape\n",
    ")\n",
    "\n",
    "# The target policy was to stick only on a sum of 20 or 21, as in Example 5.1.\n",
    "target_policy = stand_on_20\n",
    "\n",
    "# The value of this state under the target policy is approximately −0.27726 \n",
    "# (this was determined by separately generating one-hundred million episodes \n",
    "# using the target policy and averaging their returns).\n",
    "episodes = 1_000_000\n",
    "V, _ = mc.V_predict_ev(env, target_policy, episodes, start=start)\n",
    "V[start]"
   ]
  }
 ]
}